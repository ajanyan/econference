\documentclass[10pt,a4paper,journal]{IEEEtran}
\usepackage{graphicx,subfigure}
%\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\bibliographystyle{IEEEtran}
\usepackage[numbers]{natbib}
\renewcommand{\bibfont}{\normalsize}

\usepackage{mathptmx}
\usepackage{amsfonts}
\usepackage{makeidx}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{url}
\usepackage{algorithm}
\bibliographystyle{apacite}
\usepackage{algorithmic}


\newcommand\Mark[1]{\textsuperscript#1}
\usepackage[T1]{fontenc}
\renewcommand{\figurename}{\bfseries\fontsize{10}{20}\selectfont \textbf{Figure } }


\linespread{1.1}
%\setlength{\columnsep}{.5em}
\usepackage[T1]{fontenc}
%Page Number
\usepackage{nopageno}
\usepackage[left=0.75cm,right=0.75cm,top=2cm,bottom=2cm]{geometry}
\setlength{\columnsep}{2.5em}
\title{PRIVACY PRESERVING SECURITY MEASURES IN BIG DATA STORAGE}
\author{\IEEEauthorblockN{Megala G\Mark{1},
Maya Mohan\Mark{2}, and Sruthy Manmadhan\Mark{3}}\\
\Mark{1}M.Tech First Year Student,
\Mark{2}\Mark{,}\Mark{3}Assistant Professor\\
\IEEEauthorblockA{Department of Computer Science and Engineering,\\
N.S.S College of Engineering, Palakkad \\
Email: \Mark{1}megalaguruvayurkutty1@gmail.com,
\Mark{2}mayajeevan@gmail.com,
\Mark{3}sruthym.88@gmail.com }}%\author{Megala G \\MTech, Dept. of Computer Science \& Engineering \\Maya Mohan, Assistant Professor, Dept. of Computer Science and Engineering\\Sruthy Manmadhan, Assistant Professor, Dept. of Computer Science and Engineering\\ NSS College of Engineering, Palakkad \\ Kerala, India \\ megalaguruvayurkutty1@gmail.com}
%\markboth{Department of Computer Science \& Engineerinng,~December,~2013}{}

%{Shell
%\MakeLowercase{\textit{et al.}}: A Novel Tin
%Can Link}

\begin{document}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}
\begin{abstract}
Due to the advancement in technology, the amount of data generated by internet, various social networking sites, sensor networks, healthcare applications are rapidly increasing day by day. All these enormous quantities of data produced from various sources in multiple formats with very high speed is referred as Big data. Due to enormous quantity of big data, service providers depends on professionals or speacialized tools to analyze such data. Hence Privacy is one of the major challenges in big data when revealing data for third party to analyze. 

\hspace{2em}The core  applications of big data analysis require sharing of information which leads to challenge privacy. Thus proper preventive measures are necessary inorder to preserve sensitive informations of the individuals before outsourcing or revealing the data. This paper considers privacy as the major issue in big data and provides several measures to ensure the privacy of individuals private information.
\end{abstract}
%\textbf{Keywords: }\textbf{Differential Privacy, Hiding a needle in a %Haystack, Anonymization, Hadoop Distributed File System, Proxy Re-%encryption}.
\begin{keywords}
Differential Privacy, Hiding a needle in a haystack, Data anonymization, Proxy re-encryption
\end{keywords}

\section{INTRODUCTION}
\hspace{2em}Big data is reffered as the very large data sets that have  varied and complex in nature, such that conventional data processing applications are not sufficient because of it variety, volume and velocity. Due to the advancement in technology\cite{5}, the amount of data generated by internet, various social networking sites, sensor networks, healthcare applications are rapidly increasing day by day. All these enormous quantities of data produced from various sources in multiple formats with very high speed is referred as Big data. The term big data is defined as  new generation of technologies and advancements, designed to seperate values from huge data sets by allowing high-velocity capture, discovery and analysis. These characteristics usually leads to additional difficulties in storing, analyzing and extracting results. 
 
\hspace{2em} Moreover the growth of IOT also influences the data growth, with increase of these technological changes, which leads to various challenges in big data. One such challenge is the privacy in big data\cite{7}. In short privacy is defined as the access and sharing of information.The core  applications of big data analysis require sharing of information which leads to challenge privacy. Privacy is the privilege to have some control on our personal information so that we can decide how it is gatherd and used. It is the one of the important issue, which has legal and technological implications. Most of the Businesses as well as government agencies are generating and enormously collecting large amounts of data\cite{15}. Thus the current focus on enormous quantities of data will certainly create opportunities to understand the processing of huge data sets  over numerous varying domains\cite{8}. But, these capabilities of big data come with a price; the users privacy is at danger at any cost. To ensures these privacy terms and rules are incorporated in current big data analytics and mining process. 
 
 %\hspace{2em} COMPASS \citep{1} is based upon the TrueNorth a Non-Von Neumann, modular, parallel, distributed, event-driven, scalable architecture
%inspired by the function, low power, and compact volume of the organic brain. TrueNorth is a versatile substrate for integrating spatio-temporal, real-time cognitive algorithms for multi-modal, sub-symbolic, sensor-actuator systems. TrueNorth comprises of a scalable network of configurable neurosynaptic cores. Each core brings memory ("synapses"),
%processors ("neurons"), and communication ("axons") in close
%proximity, wherein inter-core communication is carried by all-
%or-none spike events, sent over a message-passing network\citep{9}.

\hspace{2em} Though there are various techniques to provide privacy to a certain amount, gradually their demerits led to the advent of newer methods in this field.

\section{DIFFERENTIAL PRIVACY}

\hspace{2em} Differential Privacy\cite{1} is a technique that provides professionals and database analysts to obtain the necessary information from the databases that contain sensitive information of people without revealing or exposing the personal identities of the individuals. This is done by producing a little amount of distortion in the information provided by the database system. The distortion which is introduced should be large enough so that they ensures the privacy and also  should be small enough so that the information provided to analyst is still useful without data utility. When the Commonwealth of Massachusetts Group Insurance Commission (GIC) revealed their anonymous health record of clients for research inorder to benefit the society. GIC preserved some information like name, street address etc. Inorder to protect their sensitive information. But by making use of the publicly available voter database and database released by GIC, one can  successfully identify the health record of users by just comparing and analyzing them. Thus hiding some information cannot only  assures the protection of individual personal information. 

\hspace{2em} Thus Differential Privacy(DP) agrees to provide the solution for such problem. To hide an individuals identity,it does the addition of mathematical noise to a sample of the individuals usage pattern. It is mainly done to make their services better, not to collect individual users usage patterns. These mathematical noise are added to the sample using laplace distribution. Apple is starting to use DP technology to discover the usage patterns, starting with ios 10 without compromising individual privacy. As large people share the same pattern, general patterns begins to evolve, which can leads to enhance the user experience in future use. In the DP data base analyst are not allowed to have direct access to the database that contains personal information. An intermediate software is introduced between the database and the database analyst to protect the privacy. This is termed as the privacy guard.\\
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.75]{1.png}
\caption{Differential Privacy\cite{1}.}
\label{Figure 1:}
\end{center}
\end{figure}

%\hspace{2em} COMPASS enables large-scale simulations that provide brain like
%function as a precursor to demonstrating the same functionality
%on TrueNorth hardware. Our goal is not to use
%COMPASS to model the brain, but to approximate brain-like
%function that integrates multiple sensory-motor modalities.  The form that  have
%chosen to concentrate on in this work leverages the largest
%known long-distance wiring diagram in the macaque brain that
%spans cortex, thalamus, and basal ganglia . The richness of the wiring diagram challenges
%the communication and computational capabilities of COMPASS
%in a manner consistent with supporting brain-like networks. To
%this end, we are mentioning have built a novel parallel compiler that takes a
%high-level network description of the macaque wiring diagram
%and converts it to the parameters needed to configure a network
%of TrueNorth cores. Our compiler is sufficiently general to
%support other application-driven networks.
\hspace{2em}Fig \ref{Figure 1:} shows the various steps to be done in sequence which are defined below.
%\hspace{2em} COMPASS is indispensable for 
\begin{enumerate}

\item The database analyst will make a query to the database through the privacy guard. 
\item The privacy guard gets the query from the analyst and evaluates this query and other previous queries for privacy risk, after evaluating the privacy risk.
\item The privacy guard then gets the data from the database. 
\item Add some distortion or noise to it according to the evaluated privacy risk and finally give it back to the analyst.
%\item estimating
%power consumption
%\item hypotheses testing, verification,
%and iteration regarding neural codes and function. We have
%used COMPASS to demonstrate numerous applications of the
%TrueNorth architecture, such as optic flow, attention mechanisms,
%image and audio classification, multi-modal image audio
%classification, character recognition, robotic navigation,
%and spatio-temporal feature extraction.

\end{enumerate}

\hspace{2em}The amount of noise added in the data is directly proportional to the evaluated privacy risk. If the privacy risk is low, noise added is small enough so that it do not affect the data utility, but should be also large enough that they preserves the individual privacy contained in the database. But if the privacy risk is high then more amount of noise is added which will affect the data utility.

%\hspace{2em} As a result of these innovations, COMPASS
%demonstrates unprecedented weak scaling beyond the cat brain
%scale achieved by C2. The architecture of COMPASS stands
%in contrast to other computational neuro-scientific simulators For reference, we point readers at reviews of large-scale brain simulations .
%COMPASS is a harbinger of an emerging use of today's
%modern supercomputers for preparing the next generation
%of application-specific processors that are increasingly proliferating
%to satisfy a world that is hungering for increased
%performance and lower power while facing the projected end
%of CMOS scaling and increasing obstacles in pushing clock
%rates ever higher.

%COMPASS is fully based upon the modular architecture
%called TrueNorth that consists of an interconnected
%and communicating network of extremely large numbers of
%neurosynaptic cores \cite{3}. There is new programming paradigms and hardware implementation based upon this architecture.

%\hspace{2em} COMPASS represent a massively parallel
%and distributed architecture for computation that complement
%the modern Von-Neumann architecture. To fully exploit this
%architecture, we need to go beyond the "intellectual bottleneck"
 %of long, sequential programming to short, parallel
%programming.

%\hspace{2em} COMPASS is multi-threaded, massively parallel and highly
%scalable, and incorporates several innovations in communication,
%computation, and memory. On a 16-rack IBM Blue
%Gene/Q supercomputer with 262144 processors and 256 TB
%of main memory, COMPASS simulated an unprecedented 256M
%(106) TrueNorth cores containing 65B (109) neurons and 16T
%(1012) synapses. These results are 3$\times$ the number of estimated
%neurons [10] in the human cortex, comparable to the number
%of synapses in the monkey cortex, and 0.08$\times$the number of
%synapses in the human cortex. At an average neuron spiking
%rate of 8.1 Hz the simulation is only 388$\times$slower than real
%time\cite{1,11}.

%\hspace{2em} COMPASS enables large-scale simulations that provide brain like
%function as a precursor to demonstrating the same functionality
%on TrueNorth hardware. 

\section{HIDING A NEEDLE IN A HAYSTACK}
\hspace{2em} All the previously existing privacy-preserving association rule algorithms change the original transaction data by injecting distortion to the data. These techniques cause the limitation in noise addition because of the need to consider privacy-data utility trade-off. However, this work maintained the original transaction in the noised transaction with the goal as to prevent data utility deterioration while preventing the privacy violations. Hiding a needle in a haystack\cite{1} is based on the idea that finding a small class of data, that is the needles, is  very hard to find in a haystack, such as a large size of data.
 There are several assumptions we adopt at first, we assume that service providers want to use the external cloud service which is basically based on Hadoop framework to achieve association rule mining for big data. Second, is that the data providers want to prevent privacy violations when the external cloud services process their data. Also that external cloud services as the third party which cannot be trusted\cite{11}.
%\begin{figure}[htbp]
%\centering
%\includegraphics[scale=.5]{4i.png}
%\caption{Conceptual architecture of a neurosynaptic core}
%\label{1}
%\end{figure}

\hspace{2em}Next assumption is that service providers want to consider the data utility for privacy preserving. However, all previous methods to preserve the privacy violation cannot overcome this  trade-off. If user want much more stronger privacy protection degree, data utility is degraded, and if user requires the more accurate analysis, it causes the chances of privacy violations.
\begin{figure}[hbtp]
\includegraphics[scale=0.90]{22.PNG}
\caption{Process of association rule mining in Hiding a needle in a haystack\cite{1}.}
\label{Figure 2:}
\end{figure}


\hspace{2em}Fig \ref{Figure 2:} shows the process of association rule mining in which service provider does the addition of dummy item as noise to the original data gathered by the data provider. At the same time, a unique code is assigned to the dummy and the original items. The service provider maintains this information to filter the dummy item after the extracting the frequent item by the external cloud platform. There are two types of code information bit string code and prime key code. In bit string code scheme, code consists of bit string such as 00001, 00011 assign to item. Prime number-based code usually assigns unique prime number to the item and then modify the code of the frequent item set by multiplying each item prime number code. Both of bit string and prime number-based code are used as a key for noise filtering. 

\hspace{2em} Apriori algorithm is done at the cloud platform using data which is given by the service provider. The external cloud platform returns back the frequent item set can also support value to the service provider. The service provider filter out the frequent item set that is affected by the dummy item using code information to extract the correct association rule. The process of extraction association rule is not a complex task for the service provider, while consider the amount of calculation required is not much.


%\hspace{2em} To ensure one-to-one equivalence between TrueNorth and
%COMPASS, there is  judicious care in the design of
%both systems. For example, in the system , bypassed the complexity
%of analog neurons traditionally associated with neuromorphic
%systems. Also, adopted pseudo-random number generators
%with configurable seeds. As a result, COMPASS has
%become the key contract between our hardware architects and
%software algorithm/application designers.

%Neurons represent computation, synapses represent memory,
%and neuron-axon connections represent communication. Each
%TrueNorth core brings computation and memory into extreme
%proximity, breaking the von-Neumann bottleneck. Note that
%synaptic or neuronal state never leaves a TrueNorth core and
%only spikes ever leave or enter any TrueNorth core. The
%communication network is driven solely by spike events, and
%requires no clocks. The neuron parameters, synaptic crossbar,
%and target axon for each neuron are reconfigurable throughout
%the system,as shown in Figure {\ref{2}}.



%\begin{figure}[htbp]
%\centering
%\includegraphics[scale=.5]{r1.png}
%\caption{Neurosynaptic Core Implementation}
%\label{2}
%\end{figure}

\section{A NOVEL GROUP KEY TRANSFER  PROTOCOL FOR BIG DATA SECURITY}

\hspace{2em}Group communication is a particular type of many to many communications which goes beyond other communications. Thus confidentiality is a prime concern which is done by encryption. All members in a group share a session key, However since the group membership changes unexpected, group key need to be modified dynamically to confirm both forward and backward secrecy of group sessions. Forward secrecy is ensured if a member who has left from the group and shall not access the content of current and future group sessions. Backward secrecy is ensured for a new member should not access the content of communications of the past sessions. To achieve these objectives, always needs a one time group key such that the key is known to the present group members.
%\begin{enumerate}
%\item Incorporates central elements from neuroscience
%(digital leaky integrate-and-fire neurons, axons which are
%output lines of neurons, and synapses that are junctions
%between neurons)
%\item is scalable with developments in CMOS
%technology
%\item implements a new architecture that integrates
%computation, communication, and memory
%\item maintains
%one-to-one correspondence with software. 
%\end{enumerate}

\hspace{2em} Most of the already existing group key transfer protocols depends on a online mutually trusted key generation centre(KGC), to transfer the group key secretly to all group members. This method needs a trusted server to be set up, and it incurs communication overhead. A novel group key transfer protocol\cite{2} without an online KGC, which is based on the combination of Diffie Hellman(DH) key agreement and a perfect linear secret sharing scheme(LSS)\cite{14}. In a secret sharing scheme, a secret s is partitioned into n shareholders and  shared among them by a mutually trusted dealer such that authorized shareholders can only reconstruct the secret and unauthorized parties cannot get the secret. If unauthorized subset of shareholders does not obtain any information about the secret, then  is called perfect. It is said to be linear, if the reconstruction operations are linear. A hybrid of public key approach and a secret sharing scheme is used in this proposed protocol. There are mainly two phases.

\subsection{Secret establishment phase}
\begin{enumerate}
\item The initiator broadcasts a request containing a random number \textit{$r_{n}\inZ_{p}$}and his/her public key \textit{$puk_{n}$}  and a list of members to announce the group communication.
\item The initiator broadcasts a request containing a random number \textit{$r_{n}\inZ_{p}$}and his/her public key \textit{$puk_{n}$}  and a list of members to announce the group communication.
\item After receiving the message from each, the initiator computes \textit{$S_{i}=puk_{i}^{prk_{n}r_{i}r_{n}}mod p$} and if result is valid, then initiator believes secret is shared with corresponding group member. otherwise the initiator claims that \textit{$i$} is fraudlent and then restarts the protocol.
\end{enumerate}
\subsection{Session key Transfer phase}
\begin{enumerate}
\item The initiator randomly generates a session key \textit{$K_{G}\inZ_{p}$}. Then, initiator computes \textit{$n-1$} additional values \textit{$U_{i}=(K_{G}-K_{i})mod p$} for each group member where \textit{$K_{i}=(S_{i}.r_{i}$} and broadcasts \textit{$U_{i}$} to all group members.
\item For each group member except the initiator, knowing the public value \textit{$U_{i}$} is able to compute the\textit{$K_{i}$}and recover the group key \textit{$K_{G}=(U_{i}+K_{i})mod p$}. Thus session key is established among all group members.
\end{enumerate}
%\hspace{2em} The basic building blocks of our neurosynaptic core are
%axons, synapses, and neurons Within the core, information
%flows from axons to neurons modulated by synapses as in figure 5.1.

%\begin{table}
%\begin{tabular}{||c|c|c||}
%\hline 
%\rule[-1ex]{0pt}{2.5ex} Name & Description & Range \\ 
%\hline 
%\rule[-1ex]{0pt}{2.5ex} $W_{i,j}$ & Connection between axon j and i & 0,1 \\ 
%\hline 
%\rule[-1ex]{0pt}{2.5ex} $G_{j}$ & Axon type & 0,1,2 \\ 
%\hline 
%\rule[-1ex]{0pt}{2.5ex} $S_{i}^{0.2}$  & Synapse Values & -256 to 255 \\ 
%\hline 
%\rule[-1ex]{0pt}{2.5ex} $L_{i}$ & Leak & -256 to 255 \\ 
%\hline 
%\rule[-1ex]{0pt}{2.5ex} $\theta_{i}$ & Threshold & 1 to 256 \\ 
%\hline 
%\end{tabular} 
%\caption{Neurosynaptic Core Architecture and Implementation}
%\label{3}
%\end{table}

%\begin{figure}[htbp]
%\begin{center}
%\hbox{\includegraphics[scale=.5]{fig/1.png}}
%\caption{neurosynaptic core}
%\label{3}
%\end{center}
%\end{figure}

\hspace{2em} Adding or removing any user does not require the modification of any existing shared secret. However, for publishing a secret group key, initiator requires to broadcast a message at the time itself each group member need to recover the group key as a function of their own random number and secret shared between them and initiator.Thus the Proposed protocol does not rely on a KGC and it can reduce the overhead of system implementation.

%\hspace{2em} Each axon corresponds to a neuron's output that could either
%reside on the core or somewhere else in a large system with
%many cores. At each time step \textit{t}, each axon \textit{j} is presented with
%an activity bit $A_{j}(t)$ that represents whether its corresponding
%neuron fired in the previous time step. Axon \textit{j} is statically
%designated as one of three types $G_{j}$ , which assumes a value
%of 0, 1, or 2; these types are used to differentiate connections with different efficacies.
%Correspondingly, neuron \textit{i} weighs synaptic input from axon \textit{j}
%of type $ G_{j} \in {0,1,2} $  as $S_{i}^{G^{j}}$ Thus, neuron i receives the
%following input from axon \textit{j}
%\begin{equation}
%A_{j}(t) \times W_{ij} \times S_{i}^{G^{j}} 
%\end{equation}


%\subsection{Event Driven Implementation}
%\hspace{2em} To implement the above specification in hardware,uses the help of  non-trivial trade-offs between power, performance,
%and density. For the current design, chooses to
%minimize active power consumption, meet (or exceed) real time
%performance, but did not aggressively pursue density
%optimizations. First, our strategy to reduce active power is to Based on these considerations and  arrived at a block-level
%implementation of our neurosynaptic core that consists of
%an input decoder with 1024 axon circuits, a 1024 $\times$ 256
%SRAM crossbar, 256 neurons, and an output encoder (Figure {\ref{4}}).
%Communication at the input and output of the core follows
%an address-event representation (AER), which encodes binary
%activity, such as A(t), by sending the locations of active
%elements via a multiplexed channel [4]. For each time step,
%the detailed operation of the core commences in two phases:
%the first phase implements the axon-driven component, and the
%second phase implements a time step synchronization.

%\hspace{2em} In the first phase, address-events are sent to the core one
%at a time, and these events are sequentially decoded to the
%appropriate axon block (e.g., axon 3 from Figure{\ref{3}}). On receiving
%an event, the axon activates the SRAM'S row, which reads
%out all of the axon's connections as well as its type. All
%the connections that exist (all the 1's) are then sent to their
%respective neurons, which perform the appropriate membrane
%potential updates; the 0's are ignored. After the completion
%of all the neuron updates, the axon block de-asserts its read,
%and is ready to process the next incoming address event; this
%continues until all address events for the current time step are
%serviced.

%\hspace{2em} second phase, which occurs once every millisecond,
%a synchronization event (Sync) is sent to all the neurons. On
%receiving this synchronization, each neuron checks to see if its
%membrane potential is above threshold, and if so, it produces a
%spike and resets the membrane potential to 0; these spikes are
%encoded and sent off as address events in a sequential fashion.
%After checking for a spike, the leak is applied.

%\hspace{2em} The purpose of breaking neural updates into two phases
%is to ensure that the hardware and software are always in
%lock step at the end of each time step. Specifically, the order
%in which address events arrive to the core or exit the core
%can vary from chip to chip due to resource arbitration-
%especially when events are sent through a non-deterministic
%routing network. To remain one-to-one, the different orderings
%must not influence the spiking dynamics. The system uses one-to one
%equivalence by first accounting for all the axon inputs,
%and then checking for spikes after these inputs have been
%processed. This also gives us a precise bound for operating in
%real time: all address events must be accounted for before the
%synchronization event, which  triggers once per millisecond.

%\subsection{Chip Design for Networks of Spiking Neurons}
%\hspace{2em} A network of spiking neurons is comprised of neuron
%processing elements connected through synapse memory elements
%%- both akin to the basic building blocks of the brain.
%Neurons communicate through binary messages called spikes,
%typically produced at 1-10 Hz. A spike is sent on a generating
%neuron's axon to the dendrites of other neurons. The point of
%contact between an axon and dendrite is the synapse, which
%has a particular strength that determines the efficacy of a spike
%from a source, pre-synaptic neuron, on a target, post-synaptic
%neuron. Each neuron has a membrane potential that changes
%with incoming spikes and decays over time due to a leak. If
%the membrane potential exceeds a threshold, the neuron generates
%its own spike and the membrane potential is reset. Each
%neuron can be excitatory or inhibitory. Spikes from excitatory
%neurons increase the membrane potential of post-synaptic neurons,
%while spikes from inhibitory neurons do the opposite.

%\begin{figure}[htbp]
%\begin{center}
%\hbox{\includegraphics[scale=.3]{r2.png}}
%\caption{Top-level block diagram of on-chip learning chip}
%\label{4}
%\end{center}
%\end{figure} 

%\hspace{2em} Networks of spiking neurons exhibit the ability to learn
%and adapt through the evolution of synapse weights. Such synaptic
%plasticity can be governed by time-dependent learning
%rules such as STDP, in which synapse weight changes depending
%on the order and distance in time between the spikes of
%source and target neurons. When a neuron fires, all incoming
%dendritic and outgoing axonal synapses must be updated.

%\hspace{2em} The neuromorphic chip implemented in this work,
%a scalable building block for large-scale networks of spiking
%neurons, integrates 256 neurons with $256\times256$ binary synapses
%to represent a fully recurrent network. In each time step, neurons
%that have exceeded their firing threshold generate spikes,
%which triggers synaptic integration in all target neurons as
%well as synapse weight updates based on the specified learning
%rule. Input spikes to the system can represent external stimuli
%(e.g. an input pattern) while generated spikes can represent
%output activity (e.g. recognition of a given pattern).

%\subsection{Transposable SRAM Synapse Crossbar Array}
%\hspace{2em} An NxN crossbar structure can effectively represent a system
%of N neurons and N2 possible synaptic connections between
%them. Each row of the crossbar represents a neuron's
%axon and each column represents a neuron's dendrite. In stark
%contrast to traditional VLSI circuits, a large fan-in and fan-out
%can thus be efficiently realized as neuronal computations and
%synaptic updates can be parallelized, since an entire row or
%column of the crossbar can be operated upon simultaneously.
%To efficiently implement time-dependent learning rules, both
%row and column accesses are important, since pre-synaptic
%row and post-synaptic column updates are required. Conventional
%memory arrays are accessed only in rows, and column based
%access would require inefficient serial row operations.
%Instead, this shortcoming is addressed in this work by a transposable
%SRAM cell (Figure {\ref{4}}) to store synapse weights, which
%enables single-cycle write and read access in both row and column directions to significantly enhance on-chip learning
%performance. The transposable SRAM cell uses 8 transistors,
%adding two access transistors connected to word and bit lines
%in transposed orientations to a standard 6T design. The cell
%area is $1.6Î¼m^{2}$ in 45nm and is fully compatible with logic
%ground rules to enable excellent low voltage yield.

%\begin{figure}[htbp]
%\begin{center}
%\hbox{\includegraphics[scale=.75]{fig/3.png}}
%\caption{Digital CMOS neuron with reconfigurability}
%\end{center}
%\end{figure}
\section{PRIVACY AND UTILITY PRESERVING DATA CLUSTERING FOR DATA ANONYMIZATION AND DISTRIBUTION ON HADOOP}
\hspace{2em}  Data anonymization is one of the advancing technique in the field of privacy-preserving data mining used to protect users privacy. Linking attack or identity disclosure is the possibility that an attacker reveals the personal attribute  of a person with the known information. K-anonymization is the process of anonymizing in a way such that the K records are indistinguishable from each other. This mechanism protects the record from identity disclosure or linking attack but are comprimised to probabilistic and various other attack which are listed below.
\begin{enumerate}
\item Homogeneity Attack: When all the records in an anonymized group contain identical sensitive attribute, the process of k-anonymization would become waste of time. This form of attack is called homogeneity attack.
\item Similarity attack: The values in an anonymized group are not identical to each other,but the values may be similar to each other.
\item Background Knowledge Attack: An intruder already knowing some background information the data uses it to inorder to narrow down the possible sensitive values in an equivalence class group.
\item Probabilistic Inference Attack: When any one of the sensitive attribute value is occurring more rapidly than the others in an group, there is a possibility for probabilistic inference attack. 
\end{enumerate}
\hspace{2em} These attacks are overcome by the proposed method to distribute the records with sensitive value equally to all the equivalence classes based on K-Nearest neighbour method. The(G,S) clustering algorithm determines the single best neighbour of each cluster and assigns instances one at a time to the existing cluster. We have changed the algorithm to overcome the skewness in the sensitive value distribution of the resultant clusters by using K-Nearest Neighbour technique. The KNN-(G,S) clustering algorithm\cite{3} finds the KN nearest neighbours from each sensitive value group and then adds the KN records to the clusters at a time. This overcomes the case of probabilistic inference attack. The anonymized data set is then distributed on a Hadoop\cite{10}. 
\begin{figure}[htbp]
\begin{center}
\includegraphics[scale=0.5]{capturerecent.png}
\caption{Privacy-preserved data distribution and datamining on Hadoop\cite{3}.}
\label{Figure 3:}
\end{center}
\end{figure}

\hspace{2em}Fig \ref{Figure 3:} describes the privacy preserved data distribution firstly, the data owners apply the KNN-(G,S) clustering algorithm on the input data seperately and distribute the data on Hadoop. Then any  kind of data mining algorithm can be applied on these distributed data using Mapreduce task in parallel patterns obtained are shared to the data owners. Any attacker who can view the privacy-preserved data on Hadoop cannot map any data to a particular individual. Thus proposed clustering algorithm can a mechanism to store the data in a privacy-preserving manner on  Hadoop cluster without causing complexity and communication overhead. The client requesting for a data, request a message in O(1) and the response would be  in O(n) where n is the dataset size.
\section{Proxy Re-Encryption: Analysis of Constructions and its Application to Secure Access Delegation}
\hspace{2em}The growth of the cloud computing has brought great expectations regarding performance, simplifying the business processes, and reduction in cost. At the same time, these advancements come with new security and privacy risks\cite{9}. Threat scenarios occur when moving from resources fully controlled by the data owner to resources processed by third party entities like public clouds. Cloud providers are assumed to be semi-trusted, because their functions are presumed correct with respect to protocol service, but they may have some incentive to read users data without their consent. This kind of approach in cloud is usually called honest-but-curious.
\subsubsection{Secure Access Delegation Scenario}
The need arise when traditional security assumptions get weakened which governs the current security architectures of cloud systems leads to the encryption of data prior to outsourcing as an essential requirement. At that time, it is also important to delegate access for sharing purposes, which is one of the most necessary functionalities. We refer this scenario as the secure access delegation scenario. There are three main separate roles: data producers, data owner, and data consumers. The most generic usage relation scenario is that multiple data producers can generate data which is owned by a data owner, who can share it with multiple data consumers.

\hspace{2em}A best solution for the secure access delegation problem to use conventional encryption technique(AES, RSA) and to share the decryption key. Symmetric encryption cannot be used alone,because same key is shared between producers, owner and consumers or, producers and owner agree on a key, which is extremely inefficient. While with public-key cryptography, the problem is that the producers do not necessarily  need to known in advance who are the intended consumers. Thus it require, the only possibility is that they encrypt the data under some common public key which is  controlled by the data owner(public key) this require to data owner has to decrypt the data and subsequently encrypt it with a key known by the intended consumers; this decrypt-and-encrypt solution requires the data owner to be online to re-encrypt the data when needed, which is extremely difficult.
The basic concept of a proxy re-encryption\cite{4} scheme is consists the ability of a proxy to transform ciphertexts which are encrypted by the public key of Alice into ciphertexts  which can be decryptable by Bob; to do so, the proxy must be in stage to re-encrypt using re-encryption key\cite{13}.In addition, the proxy should not learn any information about the encrypted messages. Fig \ref{Figure 4:} shows the actors who participate in proxy re-encryption.
\begin{figure}[hbtp]
\centering
\includegraphics[scale=1]{4.png}
\caption{Main actors and interactions in an proxy re-encryption\cite{4}.}
\label{Figure 4:}
\end{figure}
\begin{enumerate}
\item Delegator: This actor is responsible to delegates his decryption rights using proxy re-encryption. In order to do this he creates a re-encryption key, which is sends to the proxy. We refer Alice as the delegator.
\item Delegatee: The delegatee is data consumer who is granted a delegated right to decrypt ciphertexts that, although not provided for him in the first place, but re-encrypted for him with permission from the original recipient(i.e., the delegator). This actor usually refer as Bob.
\item Proxy: It handles the re-encryption process that transforms ciphertexts which are encrypted by the public key of Alice into ciphertexts  which can be decryptable by Bob without learning anything.
\end{enumerate}
\hspace{2em}In a PRE solution\cite{6}, private data is first encrypted by a data producer(which can be any entity that has the proper public key) and outsourced to a semi-trusted proxy(i.e., storage provider in the cloud). By generating the corresponding re-encryption keys and send it to the proxy, the data owner is inturn authorizing data consumers to access his data. The proxy ensures the secure access delegations through the re-encryption process using the re-encryption keys, while the information that is protected  with respect to unauthorized parties and the proxy itself\cite{12}.
\section{Discussions and Findings}

\begin{table}
\begin{center}
\caption{Comparison Table}
\label{Table 1:}
\begin{tabular}{l{l}l}
\hline
TECHNIQUE&ADVANTAGES& DISADVANTAGE\\
\hline
Differential Privacy & Privacy to identity& Affect data utility\\
Hiding Needle in Haystack & High level of Privacy& High computation cost \\
Group key Transfer Protocol & Key freshness&Theoretically secure\\
Data anonymization & Withstand Various attacks & Running time is high\\
Proxy re-encryption& Against ciphertext attack & Searching is complex\\
\hline
\end{tabular}
\end{center}
\end{table}




\hspace{2em} Table \ref{Table 1:} shows the comparison of various techniques which has been discussed in this paper. Each method has it's own situation where it can be used. Differential Privacy obscure an individual's identity, by adding mathematical noise to a sample of the individual's usage pattern. This is done by adding distortion to the information. If the privacy risk is low, noise added is small so that it do not affect data utility, but should be large enough that they protect the individual privacy of database. But if the privacy risk is high then more noise is added. So that it affect the Quality of answer. Thus it bypass the data utility of answer when privacy risk is high. Hence affect the analysis done by the data base analyst.\\
\hspace{2em} Hiding a needle in a Haystack  consider both the privacy and data utility trade-off. The service provider maintains the code information to filter out the dummy item after the extraction of frequent item set by an external cloud platform. Thus the possiblity that untrusted cloud service provider infer the real frequent is considered and thus provide privacy without data utility deteoriation. But incurs additional computation cost in the addition of noise. These both techniques are usually preffered by the service providers. Group key transfer protocol for group oriented applications of big data provide a group key which should be used by current group members without an online KGC. Thus additional server implementation cost overhead can be reduced. This also avoids additional communication cost required in an online KGC. It provides key freshness, key confientiality and key authentication. Thus backward and forward secrecy is ensured in group oriented applications. But the group key transfer phase is only theoretically secure due to network vulnerabilities.\\
\hspace{2em}Privacy and utility  preserving data anonymization and distribution in Hadoop is an anonymization technique which uses KNN algorithm to determine KN nearest neighbour and distributes the sensitive values among all the cluster. This algorithm overcomes similarity, background, Homogenity and Probabilistic inference attack.Also algorithm is precise in terms of cluster size. But the otherside is that the running time of the algorithm in determing K nearest neighbour is \textit{$O(n^{2})$}.\\
\hspace{2em}Proxy re-encryption is the better solution to implement privacy in big data analytics. proxy re-encryption scheme is embodied by the ability of a proxy to transform ciphertexts under the public key of Alice into ciphertexts decryptable by Bob; to do so, the proxy must be in possession of a re-encryption key that enables this process. In addition, the proxy cannot learn any information about the encrypted messages, under any of the keys. It considers semi trusted cloud platform and provide a re encryption process. Secure access delegation is taken into consider so that  even data owner can be on offline. Thus it achieves better privacy measures in cloud when compare to others as well as taken into consider the complexity. But searching or computing on an encrypted data is slight complex.Thus the big data privacy that is access and sharing of information is considered in a broader aspect. 
\section{Conclusion}
\hspace{2em} Thus various privacy preserving measures in big data storage is discussed. Differential privacy and Hiding a needle in a Haystack are some techniques to implement privacy. Proxy re-encryption is a special type of public key encryption in untrusted cloud providers. A novel group key transfer protocol implements privacy in a group oriented applications of big data. The data anonymization and distribution in Hadoop provides anonymized data sets and distributes on a Hadoop distributed File system and thus third party infereing sensitive values are prevented. Thus the big data privacy that is access and sharing of information is considered in a broader aspect including group communication, cloud environment and Hadoop.
\begin{thebibliography}{15}
\bibitem{1}Priyank jain, Manasi gyanchandani, Nilaykhare, 2016, "Big data privacy:- a technological perspective and review", \textit{journal of big data}, volume 3, pp 1-25.
\bibitem{2}Chingfang hsu, Bing zeng , Maoyuan zhang, 2014, "A novel group key transfer for big data security", \textit{Applied mathemathics and computation}, volume 249, pp 436-443.
\bibitem{3}J.Jesu Vedha Nayahi, V. kavitha, 2016, "Privacy and utility preserving data clustering for data anonymization and distribution on hadoop", \textit{Future generation computer systems}, volume 74, pp 393-408.
\bibitem{4}David Nunez, Isaac Agudo, Javier Lopez, 2017, "Re-encryption: Analysis of constructions and its applications to secure access Delegation", \textit{Journal of network & computer applications}, volume 87, pp 193-299.
\bibitem{5}Samiya khan, Xiufeng liu, Kashish A shakil, Mansaf Alam, 2017, "A survey on scholary data: from big data perspective", \textit{Information processing and management}, volume 53, pp 923-944.
\bibitem{6} Kun Wang member ieee, Jiahw yu member ieee, song guo ,2016, "A pre authentication approach to proxy re-encryption in big data context", \textit{IEEE transactions on big data}.
\bibitem{7}Almeida  Fernando,  Calistru, Catalin, 2013, "The main challenges and issues of big data management", \textit{International Journal of Research Studies in Computing}, volume 2, pp 11-20.
\bibitem{8}Min chen, Shiwen Mao, Yunhao Liu, 2014, "Big data: A Survey", \textit{Mobile network applications}, volume 19, pp-171-209.
\bibitem{9}S. Ananthi , Anjali Periwal , Prince Mary. S,  2016, "Data Security Based On Big Data Storage", \textit{Global Journal of Pure and Applied Mathematics}, Volume 12, pp  1491-1500.
\bibitem{10}Amrit Pal, Kunal Jain, Pinki Agrawal, Sanjay Agrawal, 2014, "A Performance Analysis of MapReduce Task with Large Number of Files Dataset in Big Data Using Hadoop", \textit{Fourth International Conference on Communication Systems and Network Technologies}.
\bibitem{11}Giannis Tziakouris,  Marios Zinonos,  Tom Chothia,  Rami Bahsoon, 2016, "Asset-Centric Security-Aware Service Selection", \textit{IEEE International Congress on Big Data}.
\bibitem{12}Zheng Yan, Senior Member, IEEE, Wenxiu Ding, Xixun Yu, Haiqi Zhu, and Robert H. Deng, Fellow, 2016, "Deduplication on Encrypted Big Data in Cloud", \textit{IEEE transactions on big data}.
\bibitem{13}Hanshu Hong, Zhixin Sun, 2017, "Towards Secure Data Sharing in Cloud Computing Using Attribute Based Proxy Re-Encryption with Keyword Search", \textit{second IEEE International Conference on Cloud Computing and Big Data Analysis}.
\bibitem{14}Changxiao  Zhao, Jianhua, 2015, "Novel Group Key Transfer Protocol for Big Data Security", \textit{IEEE Conference on big data}.
\bibitem{15}Feng Xia, Wei Wang, Teshome Megersa Bekele, Huan Liu, 2017, "Big Scholary Data: A Survey", \textit{IEEE Transactions on big data}, volume 3, pp 18-35.
\end{thebibliography}
 
\end{thebibliography}



%\hspace{2em} Through this paper  wish to discuss with the importance of Cognitive Computing and introduces an model for the Cognitive Computing and a Simple simulator for the same.
%
%\hspace*{2em}
%
%\hspace*{2em} COMPASS is simulator to implement CC in application level.\cite{1} It helps to implement brain like functions in a hardware platform. COMPASS if fully based on the architecture TrueNorth, a non-von Neumann, modular,
%parallel, distributed, event-driven, scalable architecture inspired by the function, low power, and compact volume
%of the organic brain.
%
%\hspace*{2em} The biological cells in the brain called neurons have been modelled for for different mathematical functions.So our aim is to capture input-output behaviour of neuron
%and try to implement it using mathematical models.
%\section{Applications \cite{3}} 
%\begin{enumerate}
%\item Speaker Recognition 
%\item Composer Recognition
%\item Digit Recognition
%\item HMM Sequence Modeling
%\item Collision Avoidance
%\item Optical Flow
%\item Eye Detection
%\end{enumerate}

 \bibliographystyle{plain}
 
\bibliography{dna}
\end{document}
